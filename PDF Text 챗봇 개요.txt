대용량 PDF 정보 추출 챗봇

개요: 대용량 PDF 정보 추출 챗봇은 사용자의 질문에 PDF 문서 내 관련 정보를 찾아 답변을 생성하는 시스템. 대한민국 내에서 개발하기 때문에 기본적으로 한국어 지원이 되어야 함.

답변 언어: 한국어
폴더 경로: C:\Users\user\PDF_Text_Bot\
-폴더가 없다면 폴더를 생성함
기술: OCR(광학 문자 인식), RAG(검색 증강 생성)

개발 프로세스: 
1. 데이터 수집 및 전처리
2. 텍스트 추출 (OCR): pdf 내 
3. 텍스트 분할: 추출된 긴 텍스트를 의미 있는 단위(Chunk)로 분할.
4. 임베딩 및 벡터 저장소 구축: 분할된 텍스트 청크를 벡터 형태로 변환 후 벡터 데이터베이스에 저장.
5. 검색 증강 성생 (RAG)
5-1. 사용자가 질문을 입력하면 질문 또한 벡터로 변환
5-2. 벡터 데이터베이스에서 질문 벡터와 가장 유사한 텍스트 청크 벡터를 검색
5-3. 검색된 텍스트 청크를 컨텍스트 정보로 활용해 LLM이 사용자 질문에 대한 최종 답변 생성하는


주요 기술:
1. PDF 텍스트 추출 (OCR)
1-1. PyMuPDF (Fitz): 빠르고 안정적임. 한글 텍스트 추출 가능
1-2. pdfminer.six: 텍스트의 위치와 레이아웃 정보를 잘 유지하며 텍스트 추출 가능
1-3. PyPDF2: 간단한 텍스트 추출에 용이하지만, 일부 PDF 파일이나 한글 텍스트 추출에 어려움을 겪을 수 있음
1-4. pdfplumber: 테이블 형태의 데이터 추출에도 일부 기능을 제공하지만, 이번 프로젝트 범위에서는 텍스트 추출 기능에 집중합니다.

2. PDF 이미지 추출
2-1. PyMuPDF (fitz): 빠르고 안정적임. 이미지를 추출하는데 용이
2-2. Pillow (PIL): 파이썬의 표준 이미지 처리 라이브러리. 추출된 이미지를 다루거나 OCR을 위해 이미지 처리하는 데 사용
2-3. pdf2image: PDF 페이지를 이미지 파일(Pillow 객체)로 변환해 OCR 처리를 가능하게 함
2-4. Poppler: pdf2iamage가 이미지 변환하는데 필요

3. PDF 표 추출
3-1. Camelot: PDF 내의 표를 탐지하고 추출해 Pandas DataFrame 형태로 만들어줌

4. 검색 증강 생성 (RAG) 프레임 워크
4-1. LangChain: LLM을 활용한 애플리케이션 개발을 위한 프레임워크. 데이터 로드, 분할, 임베딩, 검색, LLM 연동 등 RAG의 모든 단계 지원
4-2. LlamaIndex: RAG 시스템 구축에 특화된 프레임워크. 다양한 데이터 소스와의 연결 및 고급 검색 전략 제공

5. 대규모 시스템 아키텍처
5-1. 데이터 스집 파이프라인
5-2. 전처리 시스템
5-3. 벡터 데이터베이스
5-4. LLM 통합



역할 세 분담
웹 구현
pdf 에서 txt로 추출하는 것 (청크)
txt를 임베딩하는것 (임베딩)
■ 임베딩: 데이터 처리 시 각 데이터에 일대일로 대응하는 벡터를 만듦
	□ 임베딩 유형:
		└단어 임베딩: Word2Vec, GloVe, FastText
		└문장 임베딩: Universal Sentence Encoder (USE), SkipThought, BERT, S-BERT
		└문서 임베딩: Doc2Vec, Paragraph Vectors
	□ 모델의 종류:
		① CLIP 모델: 텍스트와 이미지의 임베딩을 생성할 수 있는 사전 훈련된 신경망
			└자연어 입력 시 임베딩으로 변환해 77*768 숫자 값 목록 생성
	□ 분산 표현 벡터:
		└단어 기반 벡터, 문장 기반 벡터 구분
		└단어 기반 벡터
			-대표 예: Word2Vec, Glove, FastText
			-단어 기반 벡터의 장단점:
				-장점: 단어의 의미를 더 잘 포착 → 단어 간의 유사성을 벡터 공간에서 유의미하게 표현
				-단점: 동음이의어에 취약
		└문장 기반 벡터
			-대표 예: RNN, LSTM, Seq2Seq, BERT, S-BERT(sentence-transformer)
			-BERT: 두 문장 간의 유사도를 판단할 때 Cross-Encoder 방식을 활용
				-Cross-Encoder 특징
					①각 문장을 미리 벡터로 변환해 유사도 비교 X
					②비교할 두 문장 중 하나라도 변경되면 모델에 다시 입력
					③장점: 문장 유사도를 출력하는 품질 자체를 높일 수 있음
					④단점: 유사도 비교를 위해 C(문장 수, 2)조합의 수 만큼 계산하기에 실용성이 떨어짐
			-S-BERT(sentence-transformer): 두 문장이 동일한 Encoder에 각각 들어가 임베딩되어 벡터로 표현하는 Bi-Encoder 활용
				-Bi-Encoder 특징
					①장점: 1000개의 문장에 대해 1000번의 Inference만 실행하면 된다. 또한 문장들을 벡터로 저장해 바로 유사도 비교에 활용가능해진다.
					②단점: Cross-Encoder보다 유사도 성능이 떨어진다.
					③단점을 보완하기 위해 SentenceBert에서 제시한 학습 방법론이 있고 현재까지도 Bi-Encoder 방식을 중심으로 임베딩 기술이 발전되어 RAG에서 활용되고 있다.
	□ 문장 기반의 Semi-supervised 학습 방법
		└대량의 labeled 학습 데이터 구축은 비용&시간의 문제가 있으므로 Semi-supervised 학습으로 문장의 표현을 잘 이해한 기초 모델을 만들어 차후 labeled 데이터로 추가 학습을 진행
		└크게 Semi-supervised 학습 방법은 Self-prediction, Contrastive learning으로 구분
	□ Self-prediction
		└Autoregressive generation: 이전의 behavior를 통해 미래의 behavior를 예측. 주어진 문장에서 다음에 올 단어를 예측하는 방식을 취한다.
			예시) GPT의 자기회귀 예측
		└Masked prediction: 정보의 일부를 마스킹하고 마스킹되지 않은 부분을 통해 마스킹된 영역을 예측한다.
			예시) BERT의 MLM
		└Innate relationship prediction: 오리지날 데이터에 변형을 가하고, 그 변형된 정보가 무엇인지 예측하는 방식. 주로 이미지에서 많이 활용된다.
			예시) equivariant-SSL
		└Hybrid self-prediction: 위의 3가지 방식을 혼합해 활용하는 방식
			예시) DALL-E
			
■ 임베딩 모델 추천: 비슷한 의미의 문장들을 벡터 공간에서 가깝게 위치시켜, 사용자 질문과 가장 연관성 높은 문서 조각(Chunk)을 효과적으로 찾아내기
	└Sentence-Transformers: RAG를 시작할 때 비용과 성능이 균형맞음
		-모델명: ko-sbert-v1 또는 ko-sroberta-multitask
		-특징: 오픈소스 및 무료, 간편 사용법, 한국어 문서 처리 고성능
		
■ RAG → Fine-tuning의 한계?
	□ RAG: '아키텍처 개념' 또는 '시스템 구성 방식'
	□ 파인튜닝: RAG 시스템 내에서 활용될 수 있는 LLM 자체를 학습시키는 방식
	□ RAG는 정보를 ‘찾아와서’ 답하는 구조, 파인튜닝은 정보를 ‘기억해서’ 답하게 만드는 구조
		
//////////////////////////////
//코드 (Gemini)
//////////////////////////////
from langchain_community.embeddings import HuggingFaceEmbeddings

# 모델을 Hugging Face에서 다운로드하여 메모리에 로드합니다.
# device='cuda'는 GPU 사용을 의미하며, GPU가 없으면 'cpu'로 설정합니다.
model_name = "jhgan/ko-sbert-v1" 
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True} # 벡터 정규화는 성능에 도움을 줍니다.

# LangChain에서 사용하기 쉬운 형태로 임베딩 모델 객체 생성
hf_embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# 텍스트를 벡터로 변환 테스트
text = "오늘 날씨가 정말 좋네요."
vector = hf_embeddings.embed_query(text)

print(f"벡터 차원: {len(vector)}")
# print(vector[:5]) # 벡터의 일부 출력
	
	
■ 참고 레퍼런스
	□ 위키독스: Transformer 강좌
	https://wikidocs.net/book/8056
	□ IntroduceAI: RAG와 LLM 그리고 임베딩(Embedding) 모델의 동향
	https://introduce-ai.tistory.com/entry/RAG%EB%A5%BC-%ED%86%B5%ED%95%B4-LLM%EC%9D%84-%EA%BD%83-%ED%94%BC%EC%9A%B0%EA%B2%8C%ED%95%98%EB%8A%94-%EC%A1%B0%EC%97%B0-%E2%80%98%EC%9E%84%EB%B2%A0%EB%94%A9-%EB%AA%A8%EB%8D%B8%E2%80%99%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%ED%9D%98%EB%9F%AC%EA%B0%80%EA%B3%A0-%EC%9E%88%EC%9D%84%EA%B9%8C
	□ 정팽팽: Sentence-BERT
	https://until.blog/@weonyee/-nlp--sentence-bert
	□ 한국어 사전학습 모델을 활용한 문장 임베딩
	https://github.com/jhgan00/ko-sentence-transformers
	
	
	
	
	
	
	
	
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■	
■ 프론트엔드
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
■ 구성 요소
	└npm install -D tailwindcss postcss autoprefixer
	□ tailwindcss
	□ postcss
	□ autoprefixer
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■	
■ 백엔드
■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■
■ 구성 요소 pip install
	langchain
	langchain_community
	PyPDF2
	flask
	flask-cors
	chromadb
	streamlit
	sentence-transformers
	huggingface_hub
	dotenv
	google-generativeai
	(예제) langchain-openai
	(예제) pip install openai
■ 사용 기술
	□ 코드
		└Python
	□ API 서버
		└Flask
	□ 벡터 DB (둘 중 1택)
		└ChromaDB: 개인용 뉴비
		└Milvus: 대규모 고성능
	□ LLM 오케스트레이터
		└LangChain: Rag 파이프라인의 모든 과정 관리
	□ 환경
		└VSCode, Github, Notion
■ 프로젝트 폴더 구조
	my_rag_backend/
(O)	├── venv/                      # 가상환경 폴더 (자동 생성됨)
	├── chroma_db/                 # ChromaDB 데이터 저장 폴더 (코드 실행 시 자동 생성됨)
	│
	├── data/                      # 원본 데이터 폴더
	│   └── sample_data.txt        # RAG 모델 학습에 사용할 텍스트 데이터
	│
(O)	├── rag_core/                  # 핵심 로직 폴더 (RAG 파이프라인 관련)
	│   ├── __init__.py            # 이 폴더를 파이썬 패키지로 인식하게 하는 빈 파일
	│   ├── vector_db.py           # 벡터 DB 설정 및 관리 코드
	│   └── rag_pipeline.py        # LangChain RAG 파이프라인 구성 코드
	│
(O)	├── app.py                     # Flask API 서버 실행 파일 (가장 바깥에 위치)
	├── .env                       # 환경 변수 파일 (API 키 등 민감 정보 저장)
	├── .gitignore                 # Git 버전 관리에서 제외할 파일/폴더 목록
	└── requirements.txt           # 프로젝트 의존성(라이브러리) 목록 파일
■ 검색 증강 생성(RAG) 프레임워크
	① LangChain: LLM을 활용한 애플리케이션 개발을 위한 프레임워크. 데이터 로드, 분할, 임베딩, 검색, LLM 연동 등 RAG의 모든 단계 지원
		-참고 레퍼런스
			└LangChain 공식 홈페이지
			https://js.langchain.com/docs/introduction/
			└AI 모델 추론을 위한 VLLM이란 무엇인가? (feat. by Paged Attention)
			https://tristanchoi.tistory.com/651
			└LLM의 추론 성능 향상을 위한 RAG 사용 시, 알아두어야 할 것들
			https://tristanchoi.tistory.com/664
			└RAG 구현을 위한 효과적인 툴: 랭체인(LangChain)
			https://tristanchoi.tistory.com/665
			└간단한 RAG 구현
			https://beanistory.tistory.com/67
			└RAG는 무엇인가? RAG를 간단하게 구현해보자
			https://velog.io/@one_two_three/RAG%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80
	② LlamaIndex: RAG 시스템 구축에 특화된 프레임워크. 다양한 데이터 소스와의 연결 및 고급 검색 전략 제공
■ 서버
	□ Django Vs. Flash Vs. FastAPI
		-참고 레퍼런스
			└Django vs Flask vs FastAPI 비교
			https://ds92.tistory.com/144
			└파이썬 웹 프레임워크 입맛대로 골라보기 (Django, Flask, FastAPI 비교)
			https://heehehe-ds.tistory.com/212
	□ Flask
		-참고 레퍼런스
			└Flask 공식 홈페이지
			https://flask.palletsprojects.com/en/stable/
			└Flask Tutorial
			https://realpython.com/learning-paths/flask-by-example/
		-서버